import os
import numpy as np
from common_src.model.utils import LiDARInstance3DBoxes

import torch
from torch.utils.data import Dataset

from vod.configuration import KittiLocations
from vod.frame import FrameDataLoader, FrameTransformMatrix, homogeneous_transformation


class ViewOfDelft(Dataset):
    CLASSES = [
        "Car",
        "Pedestrian",
        "Cyclist",
    ]
    #    'rider',
    #    'unused_bicycle',
    #    'bicycle_rack',
    #    'human_depiction',
    #    'moped_or_scooter',
    #    'motor',
    #    'truck',
    #    'other_ride',
    #    'other_vehicle',
    #    'uncertain_ride'

    LABEL_MAPPING = {
        "class": 0,  # Describes the type of object: 'Car', 'Pedestrian', 'Cyclist', etc.
        "truncated": 1,  # Not used, only there to be compatible with KITTI format.
        "occluded": 2,  # Integer (0,1,2) indicating occlusion state 0 = fully visible, 1 = partly occluded 2 = largely occluded.
        "alpha": 3,  # Observation angle of object, ranging [-pi..pi]
        "bbox2d": slice(4, 8),
        "bbox3d_dimensions": slice(
            8, 11
        ),  # 3D object dimensions: height, width, length (in meters).
        "bbox3d_location": slice(
            11, 14
        ),  # 3D object location x,y,z in camera coordinates (in meters).
        "bbox3d_rotation": 14,  # Rotation around -Z-axis in LiDAR coordinates [-pi..pi].
    }
    
    def __init__(self, 
                 data_root = 'data/view_of_delft', 
                 sequential_loading=False,
                 split = 'train',
                 augmentation_pipeline = None):
        super().__init__()

        self.data_root = data_root
        assert split in [
            "train",
            "val",
            "test",
        ], f"Invalid split: {split}. Must be one of ['train', 'val', 'test']"
        self.split = split
        split_file = os.path.join(data_root, "lidar", "ImageSets", f"{split}.txt")

        with open(split_file, "r") as f:
            lines = f.readlines()
            self.sample_list = [line.strip() for line in lines]

        self.vod_kitti_locations = KittiLocations(root_dir=data_root)
        self.augmenter = augmentation_pipeline


    def __len__(self):
        return len(self.sample_list)

    def __getitem__(self, idx):
        num_frame = self.sample_list[idx]
        vod_frame_data = FrameDataLoader(
            kitti_locations=self.vod_kitti_locations, frame_number=num_frame
        )
        local_transforms = FrameTransformMatrix(vod_frame_data)

        lidar_data = vod_frame_data.lidar_data

        gt_labels_3d_list = []
        gt_bboxes_3d_list = []

        if self.augmenter and self.split == 'train':
            calib_dict = {
                "P2": vod_frame_data.kitti_calib.P2,
                "Tr_velo_to_cam": vod_frame_data.kitti_calib.T_camera_lidar,
                "R0_rect": vod_frame_data.kitti_calib.R_rect_00
            }
            data_sample_for_aug = {
                'pc': vod_frame_data.lidar_data.copy(),
                'labels': vod_frame_data.raw_labels.copy(),
                'calib': calib_dict
            }
            augmented_sample = self.augmenter(data_sample_for_aug)
            lidar_data = augmented_sample['pc']
            raw_labels = augmented_sample['labels']
        else:
            lidar_data = vod_frame_data.lidar_data
            raw_labels = vod_frame_data.raw_labels

        if self.split != 'test':
            raw_labels = vod_frame_data.raw_labels
            for idx, label in enumerate(raw_labels):
                label = label.split(" ")

                if label[self.LABEL_MAPPING["class"]] in self.CLASSES:

                    gt_labels_3d_list.append(
                        int(self.CLASSES.index(label[self.LABEL_MAPPING["class"]]))
                    )

                    bbox3d_loc_camera = np.array(
                        label[self.LABEL_MAPPING["bbox3d_location"]]
                    )
                    trans_homo_cam = np.ones((1, 4))
                    trans_homo_cam[:, :3] = bbox3d_loc_camera
                    bbox3d_loc_lidar = homogeneous_transformation(
                        trans_homo_cam, local_transforms.t_lidar_camera
                    )

                    bbox3d_locs = np.array(bbox3d_loc_lidar[0, :3], dtype=np.float32)
                    bbox3d_dims = np.array(
                        label[self.LABEL_MAPPING["bbox3d_dimensions"]], dtype=np.float32
                    )[
                        [2, 1, 0]
                    ]  # hwl -> lwh
                    bbox3d_rot = np.array(
                        [label[self.LABEL_MAPPING["bbox3d_rotation"]]], dtype=np.float32
                    )

                    gt_bboxes_3d_list.append(
                        np.concatenate([bbox3d_locs, bbox3d_dims, bbox3d_rot], axis=0)
                    )

        lidar_data = torch.tensor(lidar_data)

        if gt_bboxes_3d_list == []:
            gt_labels_3d = np.array([0])
            gt_bboxes_3d = np.zeros((1, 7))
        else:
            gt_labels_3d = np.array(gt_labels_3d_list, dtype=np.int64)
            gt_bboxes_3d = np.stack(gt_bboxes_3d_list, axis=0)

        gt_bboxes_3d = LiDARInstance3DBoxes(
            gt_bboxes_3d, box_dim=gt_bboxes_3d.shape[-1], origin=(0.5, 0.5, 0)
        )

        gt_labels_3d = torch.tensor(gt_labels_3d)

        # image_data = vod_frame_data.image
        image_data = torch.tensor(vod_frame_data.image)

        return dict(
            lidar_data=lidar_data,
            image_data=image_data,
            transforms=local_transforms,
            gt_labels_3d=gt_labels_3d,
            gt_bboxes_3d=gt_bboxes_3d,
            meta=dict(num_frame=num_frame),
        )
